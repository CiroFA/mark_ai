top of the superior quality and openness of Arista EOS, we deliver high performance, industry-leading capacity, ultra-low latency, rich features, and powerful efficient solutions to meet our customers’ demand for capacity and network speeds for both front-end and back-end storage, compute and AI zones.  Our core switching portfolio contains both fixed and modular form factors, varying port configurations and densities, and options in power delivery all driven by customer requirements.
6
Table of Contents
The Arista Etherlink AI portfolio of 800G switches, coupled with Arista's EOS innovations such as AI Analyzer along with optimal load balancing solutions, offer compelling solutions for contemporary AI applications and deployment. Arista also continues to be innovative in such areas as deep packet buffer architectures, virtual output queuing, non-disruptive upgrades, embedded optics and next-generation optics, reversible cooling and overall system power efficiency. The Arista 7800R AI Spine, 7060 AI Leaf and the Distributed Etherlink Switch ("DES") are designed to address the demanding scale and performance requirements driven by large-scale AI networks. Arista also provides solutions for compute, GPU and storage interconnects in driving AI/ML workloads, leveraging its IP/Ethernet switches to deliver unparalleled performance and scalability.
The Arista 7700R4 DES is an ultra-scalable, intelligent distributed system engineered to meet the rigorous demands of large-scale AI and machine learning ("ML") environments. Building upon the foundations of the 7800R4 series, the 7700R4 DES delivers strong performance and scalability for accelerated computing. The Arista 7700R4 represents a significant advancement in networking technology, offering a robust and scalable solution tailored for the most demanding AI and ML workloads. Its combination of high throughput, deterministic performance, and advanced congestion management makes it an ideal choice for organizations aiming to build or expand their AI infrastructure.
AI workloads require optimized performance and availability at all times, to minimize job completion time and thus maximize utilization of expensive XPU accelerators. The EOS-based AI Agent can reside either directly on a SmartNIC or on a server CPU, to provide local configuration management of NICs along with streaming telemetry of NIC performance fed to directly-attached Arista EOS-based switches. This ensures the QoS parameters for AI optimization are consistently applied from the NIC to the network alike, to avoid misconfigurations which might cause performance bottlenecks without an easy-to-diagnose root cause. And with telemetry data spanning the AI NICs and the AI networking platforms, the network operations team can have comprehensive visibility into the entire traffic path with immediate insight into performance and problems.
Cognitive Adjacencies
Cognitive Campus Switching
- Arista’s Cognitive Campus switching products, powered by
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9f498d",
   "metadata": {},
   "source": [
    "get_company_list.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1883484a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Salvato: c:\\Users\\carbo\\OneDrive\\Desktop\\task1\\company_list.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import os\n",
    "\n",
    "BASE_DIR = os.getcwd()  # Compatibile con Jupyter\n",
    "save_path = os.path.join(BASE_DIR, \"company_list.csv\")\n",
    "\n",
    "headers = {'User-Agent': 'Gerardo DArco gerardo@email.com'}\n",
    "url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "data = response.json()\n",
    "\n",
    "with open(save_path, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"ticker\", \"cik\"])\n",
    "    for entry in data.values():\n",
    "        ticker = entry['ticker']\n",
    "        cik = str(entry['cik_str']).zfill(10)\n",
    "        writer.writerow([ticker, cik])\n",
    "\n",
    "print(f\"âœ… Salvato: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab662df1",
   "metadata": {},
   "source": [
    "download_edgar_reports.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81ee2b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] MSFT â€“ 10-K giÃ  presente\n",
      "[SKIP] MSFT â€“ 10-Q giÃ  presente\n",
      "[SKIP] AAPL â€“ 10-K giÃ  presente\n",
      "[SKIP] AAPL â€“ 10-Q giÃ  presente\n",
      "[SKIP] NVDA â€“ 10-K giÃ  presente\n",
      "[SKIP] NVDA â€“ 10-Q giÃ  presente\n",
      "[SKIP] GOOGL â€“ 10-K giÃ  presente\n",
      "[SKIP] GOOGL â€“ 10-Q giÃ  presente\n",
      "[SKIP] AMZN â€“ 10-K giÃ  presente\n",
      "[SKIP] AMZN â€“ 10-Q giÃ  presente\n",
      "[SKIP] META â€“ 10-K giÃ  presente\n",
      "[SKIP] META â€“ 10-Q giÃ  presente\n",
      "[SKIP] BRK-B â€“ 10-K giÃ  presente\n",
      "[SKIP] BRK-B â€“ 10-Q giÃ  presente\n",
      "[SKIP] AVGO â€“ 10-K giÃ  presente\n",
      "[SKIP] AVGO â€“ 10-Q giÃ  presente\n",
      "[SKIP] TSLA â€“ 10-K giÃ  presente\n",
      "[SKIP] TSLA â€“ 10-Q giÃ  presente\n",
      "[SKIP] WMT â€“ 10-K giÃ  presente\n",
      "[SKIP] WMT â€“ 10-Q giÃ  presente\n",
      "[SKIP] LLY â€“ 10-K giÃ  presente\n",
      "[SKIP] LLY â€“ 10-Q giÃ  presente\n",
      "[SKIP] JPM â€“ 10-K giÃ  presente\n",
      "[SKIP] JPM â€“ 10-Q giÃ  presente\n",
      "[SKIP] V â€“ 10-K giÃ  presente\n",
      "[SKIP] V â€“ 10-Q giÃ  presente\n",
      "[SKIP] MA â€“ 10-K giÃ  presente\n",
      "[SKIP] MA â€“ 10-Q giÃ  presente\n",
      "[SKIP] NFLX â€“ 10-K giÃ  presente\n",
      "[SKIP] NFLX â€“ 10-Q giÃ  presente\n",
      "[SKIP] COST â€“ 10-K giÃ  presente\n",
      "[SKIP] COST â€“ 10-Q giÃ  presente\n",
      "[SKIP] XOM â€“ 10-K giÃ  presente\n",
      "[SKIP] XOM â€“ 10-Q giÃ  presente\n",
      "[SKIP] ORCL â€“ 10-K giÃ  presente\n",
      "[SKIP] ORCL â€“ 10-Q giÃ  presente\n",
      "[SKIP] JNJ â€“ 10-K giÃ  presente\n",
      "[SKIP] JNJ â€“ 10-Q giÃ  presente\n",
      "[SKIP] PG â€“ 10-K giÃ  presente\n",
      "[SKIP] PG â€“ 10-Q giÃ  presente\n",
      "[SKIP] UNH â€“ 10-K giÃ  presente\n",
      "[SKIP] UNH â€“ 10-Q giÃ  presente\n",
      "[SKIP] HD â€“ 10-K giÃ  presente\n",
      "[SKIP] HD â€“ 10-Q giÃ  presente\n",
      "[SKIP] ABBV â€“ 10-K giÃ  presente\n",
      "[SKIP] ABBV â€“ 10-Q giÃ  presente\n",
      "[SKIP] BAC â€“ 10-K giÃ  presente\n",
      "[SKIP] BAC â€“ 10-Q giÃ  presente\n",
      "[SKIP] KO â€“ 10-K giÃ  presente\n",
      "[SKIP] KO â€“ 10-Q giÃ  presente\n",
      "[SKIP] PLTR â€“ 10-K giÃ  presente\n",
      "[SKIP] PLTR â€“ 10-Q giÃ  presente\n",
      "[SKIP] TMUS â€“ 10-K giÃ  presente\n",
      "[SKIP] TMUS â€“ 10-Q giÃ  presente\n",
      "[SKIP] PM â€“ 10-K giÃ  presente\n",
      "[SKIP] PM â€“ 10-Q giÃ  presente\n",
      "[SKIP] CRM â€“ 10-K giÃ  presente\n",
      "[SKIP] CRM â€“ 10-Q giÃ  presente\n",
      "[SKIP] WFC â€“ 10-K giÃ  presente\n",
      "[SKIP] WFC â€“ 10-Q giÃ  presente\n",
      "[SKIP] CVX â€“ 10-K giÃ  presente\n",
      "[SKIP] CVX â€“ 10-Q giÃ  presente\n",
      "[SKIP] CSCO â€“ 10-K giÃ  presente\n",
      "[SKIP] CSCO â€“ 10-Q giÃ  presente\n",
      "[SKIP] IBM â€“ 10-K giÃ  presente\n",
      "[SKIP] IBM â€“ 10-Q giÃ  presente\n",
      "[SKIP] ABT â€“ 10-K giÃ  presente\n",
      "[SKIP] ABT â€“ 10-Q giÃ  presente\n",
      "[SKIP] MCD â€“ 10-K giÃ  presente\n",
      "[SKIP] MCD â€“ 10-Q giÃ  presente\n",
      "[SKIP] GE â€“ 10-K giÃ  presente\n",
      "[SKIP] GE â€“ 10-Q giÃ  presente\n",
      "[SKIP] LIN â€“ 10-K giÃ  presente\n",
      "[SKIP] LIN â€“ 10-Q giÃ  presente\n",
      "[SKIP] MRK â€“ 10-K giÃ  presente\n",
      "[SKIP] MRK â€“ 10-Q giÃ  presente\n",
      "[SKIP] NOW â€“ 10-K giÃ  presente\n",
      "[SKIP] NOW â€“ 10-Q giÃ  presente\n",
      "[SKIP] T â€“ 10-K giÃ  presente\n",
      "[SKIP] T â€“ 10-Q giÃ  presente\n",
      "[SKIP] AXP â€“ 10-K giÃ  presente\n",
      "[SKIP] AXP â€“ 10-Q giÃ  presente\n",
      "[SKIP] ACN â€“ 10-K giÃ  presente\n",
      "[SKIP] ACN â€“ 10-Q giÃ  presente\n",
      "[SKIP] MS â€“ 10-K giÃ  presente\n",
      "[SKIP] MS â€“ 10-Q giÃ  presente\n",
      "[SKIP] ISRG â€“ 10-K giÃ  presente\n",
      "[SKIP] ISRG â€“ 10-Q giÃ  presente\n",
      "[SKIP] VZ â€“ 10-K giÃ  presente\n",
      "[SKIP] VZ â€“ 10-Q giÃ  presente\n",
      "[SKIP] PEP â€“ 10-K giÃ  presente\n",
      "[SKIP] PEP â€“ 10-Q giÃ  presente\n",
      "[SKIP] UBER â€“ 10-K giÃ  presente\n",
      "[SKIP] UBER â€“ 10-Q giÃ  presente\n",
      "[SKIP] INTU â€“ 10-K giÃ  presente\n",
      "[SKIP] INTU â€“ 10-Q giÃ  presente\n",
      "[SKIP] GS â€“ 10-K giÃ  presente\n",
      "[SKIP] GS â€“ 10-Q giÃ  presente\n",
      "[SKIP] RTX â€“ 10-K giÃ  presente\n",
      "[SKIP] RTX â€“ 10-Q giÃ  presente\n",
      "[SKIP] BKNG â€“ 10-K giÃ  presente\n",
      "[SKIP] BKNG â€“ 10-Q giÃ  presente\n",
      "[SKIP] DIS â€“ 10-K giÃ  presente\n",
      "[SKIP] DIS â€“ 10-Q giÃ  presente\n",
      "[SKIP] BX â€“ 10-K giÃ  presente\n",
      "[SKIP] BX â€“ 10-Q giÃ  presente\n",
      "[SKIP] PGR â€“ 10-K giÃ  presente\n",
      "[SKIP] PGR â€“ 10-Q giÃ  presente\n",
      "[SKIP] AMD â€“ 10-K giÃ  presente\n",
      "[SKIP] AMD â€“ 10-Q giÃ  presente\n",
      "[SKIP] ADBE â€“ 10-K giÃ  presente\n",
      "[SKIP] ADBE â€“ 10-Q giÃ  presente\n",
      "[SKIP] TMO â€“ 10-K giÃ  presente\n",
      "[SKIP] TMO â€“ 10-Q giÃ  presente\n",
      "[SKIP] SPGI â€“ 10-K giÃ  presente\n",
      "[SKIP] SPGI â€“ 10-Q giÃ  presente\n",
      "[SKIP] BSX â€“ 10-K giÃ  presente\n",
      "[SKIP] BSX â€“ 10-Q giÃ  presente\n",
      "[SKIP] QCOM â€“ 10-K giÃ  presente\n",
      "[SKIP] QCOM â€“ 10-Q giÃ  presente\n",
      "[SKIP] CAT â€“ 10-K giÃ  presente\n",
      "[SKIP] CAT â€“ 10-Q giÃ  presente\n",
      "[SKIP] SCHW â€“ 10-K giÃ  presente\n",
      "[SKIP] SCHW â€“ 10-Q giÃ  presente\n",
      "[SKIP] AMGN â€“ 10-K giÃ  presente\n",
      "[SKIP] AMGN â€“ 10-Q giÃ  presente\n",
      "[SKIP] TXN â€“ 10-K giÃ  presente\n",
      "[SKIP] TXN â€“ 10-Q giÃ  presente\n",
      "[SKIP] SYK â€“ 10-K giÃ  presente\n",
      "[SKIP] SYK â€“ 10-Q giÃ  presente\n",
      "[SKIP] TJX â€“ 10-K giÃ  presente\n",
      "[SKIP] TJX â€“ 10-Q giÃ  presente\n",
      "[SKIP] BLK â€“ 10-K giÃ  presente\n",
      "[SKIP] BLK â€“ 10-Q giÃ  presente\n",
      "[SKIP] DHR â€“ 10-K giÃ  presente\n",
      "[SKIP] DHR â€“ 10-Q giÃ  presente\n",
      "[SKIP] BA â€“ 10-K giÃ  presente\n",
      "[SKIP] BA â€“ 10-Q giÃ  presente\n",
      "[SKIP] HON â€“ 10-K giÃ  presente\n",
      "[SKIP] HON â€“ 10-Q giÃ  presente\n",
      "[SKIP] NEE â€“ 10-K giÃ  presente\n",
      "[SKIP] NEE â€“ 10-Q giÃ  presente\n",
      "[SKIP] PFE â€“ 10-K giÃ  presente\n",
      "[SKIP] PFE â€“ 10-Q giÃ  presente\n",
      "[SKIP] C â€“ 10-K giÃ  presente\n",
      "[SKIP] C â€“ 10-Q giÃ  presente\n",
      "[SKIP] DE â€“ 10-K giÃ  presente\n",
      "[SKIP] DE â€“ 10-Q giÃ  presente\n",
      "[SKIP] UNP â€“ 10-K giÃ  presente\n",
      "[SKIP] UNP â€“ 10-Q giÃ  presente\n",
      "[SKIP] CMCSA â€“ 10-K giÃ  presente\n",
      "[SKIP] CMCSA â€“ 10-Q giÃ  presente\n",
      "[SKIP] VRTX â€“ 10-K giÃ  presente\n",
      "[SKIP] VRTX â€“ 10-Q giÃ  presente\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Configurazione base\n",
    "BASE_DIR = os.getcwd()\n",
    "CSV_PATH = os.path.join(BASE_DIR, \"company_list.csv\")\n",
    "SAVE_FOLDER = os.path.join(BASE_DIR, \"filings\")\n",
    "FORM_TYPES = [\"10-K\", \"10-Q\"]\n",
    "MAX_COMPANIES = 100  # cambia se vuoi piÃ¹ aziende\n",
    "HEADERS = {'User-Agent': 'Gerardo DArco gerardo@email.com'}\n",
    "\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
    "\n",
    "def download_form(ticker, cik, form_type, already_files):\n",
    "    \"\"\"Scarica il primo filing disponibile (10-K o 10-Q) se non giÃ  presente\"\"\"\n",
    "    try:\n",
    "        url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "        r = requests.get(url, headers=HEADERS)\n",
    "        data = r.json()\n",
    "        recent = data[\"filings\"][\"recent\"]\n",
    "        forms = recent[\"form\"]\n",
    "        accessions = recent[\"accessionNumber\"]\n",
    "\n",
    "        downloaded = 0\n",
    "        for i in range(len(forms)):\n",
    "            if forms[i] == form_type:\n",
    "                acc_no = accessions[i].replace(\"-\", \"\")\n",
    "                filename = f\"{ticker}_{form_type}_{acc_no}.html\"\n",
    "                path = os.path.join(SAVE_FOLDER, filename)\n",
    "\n",
    "                if filename in already_files:\n",
    "                    continue  # giÃ  scaricato\n",
    "\n",
    "                filing_url = f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{acc_no}/{acc_no}-index.html\"\n",
    "                html = requests.get(filing_url, headers=HEADERS).text\n",
    "\n",
    "                with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(html)\n",
    "\n",
    "                print(f\"[DOWNLOAD] {filename}\")\n",
    "                time.sleep(0.5)\n",
    "                downloaded += 1\n",
    "                break  # solo uno per tipo (piÃ¹ recente)\n",
    "\n",
    "        return downloaded\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {ticker} â€“ {form_type} â€“ {e}\")\n",
    "        return 0\n",
    "\n",
    "def main():\n",
    "    already_files = set(f for f in os.listdir(SAVE_FOLDER) if f.endswith(\".html\"))\n",
    "\n",
    "    with open(CSV_PATH, \"r\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for i, row in enumerate(reader):\n",
    "            if i >= MAX_COMPANIES:\n",
    "                break\n",
    "            ticker = row[\"ticker\"]\n",
    "            cik = row[\"cik\"]\n",
    "\n",
    "            for form_type in FORM_TYPES:\n",
    "                prefix = f\"{ticker}_{form_type}_\"\n",
    "                already_for_type = any(f.startswith(prefix) for f in already_files)\n",
    "\n",
    "                if not already_for_type:\n",
    "                    download_form(ticker, cik, form_type, already_files)\n",
    "                else:\n",
    "                    print(f\"[SKIP] {ticker} â€“ {form_type} giÃ  presente\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d2270f",
   "metadata": {},
   "source": [
    "extract_text.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f483619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXTRACT] ACN_10-K_000146737324000278.html\n",
      "[EXTRACT] ACN_10-Q_000146737325000100.html\n",
      "[EXTRACT] ADBE_10-K_000079634325000004.html\n",
      "[EXTRACT] ADBE_10-Q_000079634325000059.html\n",
      "[EXTRACT] AMD_10-K_000000248825000012.html\n",
      "[EXTRACT] AMD_10-Q_000000248825000047.html\n",
      "[EXTRACT] AMGN_10-K_000031815425000010.html\n",
      "[EXTRACT] AMGN_10-Q_000031815425000020.html\n",
      "[EXTRACT] AXP_10-K_000000496225000016.html\n",
      "[EXTRACT] AXP_10-Q_000000496225000045.html\n",
      "[EXTRACT] BA_10-K_000001292725000015.html\n",
      "[EXTRACT] BA_10-Q_000001292725000031.html\n",
      "[EXTRACT] BKNG_10-K_000107553125000010.html\n",
      "[EXTRACT] BKNG_10-Q_000107553125000024.html\n",
      "[EXTRACT] BLK_10-K_000095017025026584.html\n",
      "[EXTRACT] BLK_10-Q_000095017025065838.html\n",
      "[EXTRACT] BSX_10-K_000088572525000011.html\n",
      "[EXTRACT] BSX_10-Q_000088572525000026.html\n",
      "[EXTRACT] BX_10-K_000119312525042469.html\n",
      "[EXTRACT] BX_10-Q_000119312525111595.html\n",
      "[EXTRACT] CAT_10-K_000001823025000008.html\n",
      "[EXTRACT] CAT_10-Q_000001823025000016.html\n",
      "[EXTRACT] CMCSA_10-K_000116669125000011.html\n",
      "[EXTRACT] CMCSA_10-Q_000116669125000021.html\n",
      "[EXTRACT] C_10-K_000083100125000067.html\n",
      "[EXTRACT] C_10-Q_000083100125000086.html\n",
      "[EXTRACT] DE_10-K_000155837024016169.html\n",
      "[EXTRACT] DE_10-Q_000155837025001714.html\n",
      "[EXTRACT] DHR_10-K_000031361625000043.html\n",
      "[EXTRACT] DHR_10-Q_000031361625000088.html\n",
      "[EXTRACT] DIS_10-K_000174448924000276.html\n",
      "[EXTRACT] DIS_10-Q_000174448925000098.html\n",
      "[EXTRACT] GS_10-K_000088698225000005.html\n",
      "[EXTRACT] GS_10-Q_000088698225000009.html\n",
      "[EXTRACT] HON_10-K_000077384025000010.html\n",
      "[EXTRACT] HON_10-Q_000077384025000037.html\n",
      "[EXTRACT] INTU_10-K_000089687824000039.html\n",
      "[EXTRACT] INTU_10-Q_000089687825000015.html\n",
      "[EXTRACT] ISRG_10-K_000103526725000017.html\n",
      "[EXTRACT] ISRG_10-Q_000103526725000109.html\n",
      "[EXTRACT] MS_10-K_000089542125000304.html\n",
      "[EXTRACT] MS_10-Q_000089542125000343.html\n",
      "[EXTRACT] NEE_10-K_000075330825000011.html\n",
      "[EXTRACT] NEE_10-Q_000075330825000024.html\n",
      "[EXTRACT] PEP_10-K_000007747625000007.html\n",
      "[EXTRACT] PEP_10-Q_000007747625000019.html\n",
      "[EXTRACT] PFE_10-K_000007800325000054.html\n",
      "[EXTRACT] PFE_10-Q_000007800325000114.html\n",
      "[EXTRACT] PGR_10-K_000008066125000007.html\n",
      "[EXTRACT] PGR_10-Q_000008066125000023.html\n",
      "[EXTRACT] QCOM_10-K_000080432824000075.html\n",
      "[EXTRACT] QCOM_10-Q_000080432825000031.html\n",
      "[EXTRACT] RTX_10-K_000010182925000005.html\n",
      "[EXTRACT] RTX_10-Q_000010182925000015.html\n",
      "[EXTRACT] SCHW_10-K_000031670925000010.html\n",
      "[EXTRACT] SCHW_10-Q_000031670925000023.html\n",
      "[EXTRACT] SPGI_10-K_000006404025000052.html\n",
      "[EXTRACT] SPGI_10-Q_000006404025000126.html\n",
      "[EXTRACT] SYK_10-K_000031076425000023.html\n",
      "[EXTRACT] SYK_10-Q_000031076425000075.html\n",
      "[EXTRACT] TJX_10-K_000010919825000010.html\n",
      "[EXTRACT] TJX_10-Q_000010919824000059.html\n",
      "[EXTRACT] TMO_10-K_000009774525000010.html\n",
      "[EXTRACT] TMO_10-Q_000009774525000041.html\n",
      "[EXTRACT] TXN_10-K_000009747625000007.html\n",
      "[EXTRACT] TXN_10-Q_000009747625000027.html\n",
      "[EXTRACT] T_10-K_000073271725000013.html\n",
      "[EXTRACT] T_10-Q_000073271725000023.html\n",
      "[EXTRACT] UBER_10-K_000154315125000008.html\n",
      "[EXTRACT] UBER_10-Q_000154315125000015.html\n",
      "[EXTRACT] UNP_10-K_000010088525000042.html\n",
      "[EXTRACT] UNP_10-Q_000010088525000151.html\n",
      "[EXTRACT] VRTX_10-K_000087532025000053.html\n",
      "[EXTRACT] VRTX_10-Q_000087532025000192.html\n",
      "[EXTRACT] VZ_10-K_000073271225000006.html\n",
      "[EXTRACT] VZ_10-Q_000073271225000024.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "INPUT_FOLDER = os.path.join(BASE_DIR, \"filings\")\n",
    "OUTPUT_FOLDER = os.path.join(BASE_DIR, \"text_clean\")\n",
    "\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "def extract_text_from_html(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "        for tag in soup([\"script\", \"style\"]):\n",
    "            tag.decompose()\n",
    "        text = soup.get_text(separator=\"\\n\")\n",
    "        lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "def process_all_files():\n",
    "    for file in os.listdir(INPUT_FOLDER):\n",
    "        if not file.endswith(\".html\"):\n",
    "            continue\n",
    "        input_path = os.path.join(INPUT_FOLDER, file)\n",
    "        output_filename = file.replace(\".html\", \".txt\")\n",
    "        output_path = os.path.join(OUTPUT_FOLDER, output_filename)\n",
    "        if os.path.exists(output_path):\n",
    "            continue\n",
    "        text = extract_text_from_html(input_path)\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "            out.write(text)\n",
    "        print(f\"[EXTRACT] {file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236350c",
   "metadata": {},
   "source": [
    "#CONTARE QUANTE AZIENDE CI SONO DA CANCELLARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcf37146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Numero di aziende uniche: 77\n",
      "Esempi di ticker: ['AAPL', 'ABBV', 'ABT', 'ACN', 'ADBE', 'AMD', 'AMGN', 'AMZN', 'AVGO', 'AXP']...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "FOLDER = \"filings\"  # o \"text_clean\"\n",
    "\n",
    "tickers = set()\n",
    "\n",
    "for file in os.listdir(FOLDER):\n",
    "    if file.endswith(\".html\") or file.endswith(\".txt\"):\n",
    "        ticker = file.split(\"_\")[0]\n",
    "        tickers.add(ticker)\n",
    "\n",
    "print(f\"âœ… Numero di aziende uniche: {len(tickers)}\")\n",
    "print(f\"Esempi di ticker: {sorted(list(tickers))[:10]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e923e67",
   "metadata": {},
   "source": [
    "chunk_text.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c1a5bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHUNKED] ACN_10-K_000146737324000278.txt â†’ 1 blocchi\n",
      "[CHUNKED] ACN_10-Q_000146737325000100.txt â†’ 1 blocchi\n",
      "[CHUNKED] ADBE_10-K_000079634325000004.txt â†’ 1 blocchi\n",
      "[CHUNKED] ADBE_10-Q_000079634325000059.txt â†’ 1 blocchi\n",
      "[CHUNKED] AMD_10-K_000000248825000012.txt â†’ 1 blocchi\n",
      "[CHUNKED] AMD_10-Q_000000248825000047.txt â†’ 1 blocchi\n",
      "[CHUNKED] AMGN_10-K_000031815425000010.txt â†’ 1 blocchi\n",
      "[CHUNKED] AMGN_10-Q_000031815425000020.txt â†’ 1 blocchi\n",
      "[CHUNKED] AXP_10-K_000000496225000016.txt â†’ 1 blocchi\n",
      "[CHUNKED] AXP_10-Q_000000496225000045.txt â†’ 1 blocchi\n",
      "[CHUNKED] BA_10-K_000001292725000015.txt â†’ 1 blocchi\n",
      "[CHUNKED] BA_10-Q_000001292725000031.txt â†’ 1 blocchi\n",
      "[CHUNKED] BKNG_10-K_000107553125000010.txt â†’ 1 blocchi\n",
      "[CHUNKED] BKNG_10-Q_000107553125000024.txt â†’ 1 blocchi\n",
      "[CHUNKED] BLK_10-K_000095017025026584.txt â†’ 1 blocchi\n",
      "[CHUNKED] BLK_10-Q_000095017025065838.txt â†’ 1 blocchi\n",
      "[CHUNKED] BSX_10-K_000088572525000011.txt â†’ 1 blocchi\n",
      "[CHUNKED] BSX_10-Q_000088572525000026.txt â†’ 1 blocchi\n",
      "[CHUNKED] BX_10-K_000119312525042469.txt â†’ 1 blocchi\n",
      "[CHUNKED] BX_10-Q_000119312525111595.txt â†’ 1 blocchi\n",
      "[CHUNKED] CAT_10-K_000001823025000008.txt â†’ 1 blocchi\n",
      "[CHUNKED] CAT_10-Q_000001823025000016.txt â†’ 1 blocchi\n",
      "[CHUNKED] CMCSA_10-K_000116669125000011.txt â†’ 1 blocchi\n",
      "[CHUNKED] CMCSA_10-Q_000116669125000021.txt â†’ 1 blocchi\n",
      "[CHUNKED] C_10-K_000083100125000067.txt â†’ 1 blocchi\n",
      "[CHUNKED] C_10-Q_000083100125000086.txt â†’ 1 blocchi\n",
      "[CHUNKED] DE_10-K_000155837024016169.txt â†’ 1 blocchi\n",
      "[CHUNKED] DE_10-Q_000155837025001714.txt â†’ 1 blocchi\n",
      "[CHUNKED] DHR_10-K_000031361625000043.txt â†’ 1 blocchi\n",
      "[CHUNKED] DHR_10-Q_000031361625000088.txt â†’ 1 blocchi\n",
      "[CHUNKED] DIS_10-K_000174448924000276.txt â†’ 1 blocchi\n",
      "[CHUNKED] DIS_10-Q_000174448925000098.txt â†’ 1 blocchi\n",
      "[CHUNKED] GS_10-K_000088698225000005.txt â†’ 1 blocchi\n",
      "[CHUNKED] GS_10-Q_000088698225000009.txt â†’ 1 blocchi\n",
      "[CHUNKED] HON_10-K_000077384025000010.txt â†’ 1 blocchi\n",
      "[CHUNKED] HON_10-Q_000077384025000037.txt â†’ 1 blocchi\n",
      "[CHUNKED] INTU_10-K_000089687824000039.txt â†’ 1 blocchi\n",
      "[CHUNKED] INTU_10-Q_000089687825000015.txt â†’ 1 blocchi\n",
      "[CHUNKED] ISRG_10-K_000103526725000017.txt â†’ 1 blocchi\n",
      "[CHUNKED] ISRG_10-Q_000103526725000109.txt â†’ 1 blocchi\n",
      "[CHUNKED] MS_10-K_000089542125000304.txt â†’ 1 blocchi\n",
      "[CHUNKED] MS_10-Q_000089542125000343.txt â†’ 1 blocchi\n",
      "[CHUNKED] NEE_10-K_000075330825000011.txt â†’ 1 blocchi\n",
      "[CHUNKED] NEE_10-Q_000075330825000024.txt â†’ 1 blocchi\n",
      "[CHUNKED] PEP_10-K_000007747625000007.txt â†’ 1 blocchi\n",
      "[CHUNKED] PEP_10-Q_000007747625000019.txt â†’ 1 blocchi\n",
      "[CHUNKED] PFE_10-K_000007800325000054.txt â†’ 1 blocchi\n",
      "[CHUNKED] PFE_10-Q_000007800325000114.txt â†’ 1 blocchi\n",
      "[CHUNKED] PGR_10-K_000008066125000007.txt â†’ 1 blocchi\n",
      "[CHUNKED] PGR_10-Q_000008066125000023.txt â†’ 1 blocchi\n",
      "[CHUNKED] QCOM_10-K_000080432824000075.txt â†’ 1 blocchi\n",
      "[CHUNKED] QCOM_10-Q_000080432825000031.txt â†’ 1 blocchi\n",
      "[CHUNKED] RTX_10-K_000010182925000005.txt â†’ 1 blocchi\n",
      "[CHUNKED] RTX_10-Q_000010182925000015.txt â†’ 1 blocchi\n",
      "[CHUNKED] SCHW_10-K_000031670925000010.txt â†’ 1 blocchi\n",
      "[CHUNKED] SCHW_10-Q_000031670925000023.txt â†’ 1 blocchi\n",
      "[CHUNKED] SPGI_10-K_000006404025000052.txt â†’ 1 blocchi\n",
      "[CHUNKED] SPGI_10-Q_000006404025000126.txt â†’ 1 blocchi\n",
      "[CHUNKED] SYK_10-K_000031076425000023.txt â†’ 1 blocchi\n",
      "[CHUNKED] SYK_10-Q_000031076425000075.txt â†’ 1 blocchi\n",
      "[CHUNKED] TJX_10-K_000010919825000010.txt â†’ 1 blocchi\n",
      "[CHUNKED] TJX_10-Q_000010919824000059.txt â†’ 1 blocchi\n",
      "[CHUNKED] TMO_10-K_000009774525000010.txt â†’ 1 blocchi\n",
      "[CHUNKED] TMO_10-Q_000009774525000041.txt â†’ 1 blocchi\n",
      "[CHUNKED] TXN_10-K_000009747625000007.txt â†’ 1 blocchi\n",
      "[CHUNKED] TXN_10-Q_000009747625000027.txt â†’ 1 blocchi\n",
      "[CHUNKED] T_10-K_000073271725000013.txt â†’ 1 blocchi\n",
      "[CHUNKED] T_10-Q_000073271725000023.txt â†’ 1 blocchi\n",
      "[CHUNKED] UBER_10-K_000154315125000008.txt â†’ 1 blocchi\n",
      "[CHUNKED] UBER_10-Q_000154315125000015.txt â†’ 1 blocchi\n",
      "[CHUNKED] UNP_10-K_000010088525000042.txt â†’ 1 blocchi\n",
      "[CHUNKED] UNP_10-Q_000010088525000151.txt â†’ 1 blocchi\n",
      "[CHUNKED] VRTX_10-K_000087532025000053.txt â†’ 1 blocchi\n",
      "[CHUNKED] VRTX_10-Q_000087532025000192.txt â†’ 1 blocchi\n",
      "[CHUNKED] VZ_10-K_000073271225000006.txt â†’ 1 blocchi\n",
      "[CHUNKED] VZ_10-Q_000073271225000024.txt â†’ 1 blocchi\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import textwrap\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "INPUT_FOLDER = os.path.join(BASE_DIR, \"text_clean\")\n",
    "OUTPUT_FOLDER = os.path.join(BASE_DIR, \"chunks\")\n",
    "MAX_CHARS = 3000\n",
    "\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "def safe_split_paragraph(para, max_chars):\n",
    "    return textwrap.wrap(para, width=max_chars, break_long_words=False, replace_whitespace=False)\n",
    "\n",
    "def chunk_by_paragraphs(text, max_chars=MAX_CHARS):\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for para in paragraphs:\n",
    "        para = para.strip()\n",
    "        if not para:\n",
    "            continue\n",
    "        if len(para) > max_chars:\n",
    "            for sub_para in safe_split_paragraph(para, max_chars):\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    current_chunk = \"\"\n",
    "                chunks.append(sub_para.strip())\n",
    "        elif len(current_chunk) + len(para) + 2 > max_chars:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = para\n",
    "        else:\n",
    "            current_chunk += \"\\n\\n\" + para\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def process_all_files():\n",
    "    for file in os.listdir(INPUT_FOLDER):\n",
    "        if not file.endswith(\".txt\"):\n",
    "            continue\n",
    "        input_path = os.path.join(INPUT_FOLDER, file)\n",
    "        base_name = file.replace(\".txt\", \"\")\n",
    "        chunk_already_present = any(f.startswith(base_name) for f in os.listdir(OUTPUT_FOLDER))\n",
    "        if chunk_already_present:\n",
    "            continue\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        chunks = chunk_by_paragraphs(text)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_filename = f\"{base_name}_chunk{i+1}.txt\"\n",
    "            output_path = os.path.join(OUTPUT_FOLDER, chunk_filename)\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "                out.write(chunk)\n",
    "        print(f\"[CHUNKED] {file} â†’ {len(chunks)} blocchi\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730653b",
   "metadata": {},
   "source": [
    "embed_and_index.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e471bb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5821dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ Calcolo 154 embedding con OpenAI (text-embedding-3-small)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 154/154 [01:05<00:00,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Indice FAISS salvato in: c:\\Users\\carbo\\OneDrive\\Desktop\\task1\\index\\company_index.faiss\n",
      "âœ… Metadati salvati in: c:\\Users\\carbo\\OneDrive\\Desktop\\task1\\index\\metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ðŸ“ Percorsi cartelle\n",
    "BASE_DIR = os.getcwd()\n",
    "CHUNKS_FOLDER = os.path.join(BASE_DIR, \"chunks\")\n",
    "INDEX_FOLDER = os.path.join(BASE_DIR, \"index\")\n",
    "os.makedirs(INDEX_FOLDER, exist_ok=True)\n",
    "\n",
    "INDEX_PATH = os.path.join(INDEX_FOLDER, \"company_index.faiss\")\n",
    "METADATA_PATH = os.path.join(INDEX_FOLDER, \"metadata.json\")\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "# ðŸ” Carica chiave OpenAI\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# ðŸ“š Carica tutti i chunk .txt\n",
    "chunks = []\n",
    "metadati = []\n",
    "\n",
    "for file in sorted(os.listdir(CHUNKS_FOLDER)):\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = os.path.join(CHUNKS_FOLDER, file)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        if text.strip():\n",
    "            chunks.append(text)\n",
    "            metadati.append({\n",
    "                \"filename\": file,\n",
    "                \"length\": len(text),\n",
    "                \"path\": file_path\n",
    "            })\n",
    "\n",
    "# ðŸ” Calcolo embedding\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=[text],\n",
    "        model=EMBEDDING_MODEL\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "print(f\"ðŸ”¢ Calcolo {len(chunks)} embedding con OpenAI ({EMBEDDING_MODEL})...\")\n",
    "embeddings = [get_embedding(chunk) for chunk in tqdm(chunks)]\n",
    "\n",
    "# ðŸ“¦ Costruzione indice FAISS\n",
    "dimension = len(embeddings[0])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings).astype(\"float32\"))\n",
    "\n",
    "# ðŸ’¾ Salva indice e metadati\n",
    "faiss.write_index(index, INDEX_PATH)\n",
    "with open(METADATA_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadati, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Indice FAISS salvato in: {INDEX_PATH}\")\n",
    "print(f\"âœ… Metadati salvati in: {METADATA_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
